项目文件：

	gradient.py		实现代码文件


注意事项：

	需要用户手动修改 gradient.py 中的__main__部分的函数名以测试不同下降方式

存在的一些问题：

	不能从原给定的初始点(-1, 0.2)到达最优点，需要自行给出一个最优点一定范围内的初始点，比如(-3, 4)

实验报告：

    由于能力有限，并没有绘制俯视视角下的点的变化情况图

    单从迭代次数上看，newton最少(5次)，ada(10)，gd(20)，adam(50)
    所以目标函数值下降过程曲线中，newton最陡，ada和adam居中，gd最为平缓

    分析算法的计算时间(去除掉辅助信息输出，单纯测量迭代时间)
    gd————55ms
    newton————9ms
    adam————100ms
    ada————50ms
    测试结果与预期相差较大，但感觉可以理解：
    - newton迭代次数少，虽然要求矩阵的逆，但这里矩阵阶数小，费用不是很大
    - adam由于我设置参数的原因，使其学习率过低
    - ada我设置了较高的学习率，降低了其迭代次数

    最后迭代结束点为(-2.9035, -2.9035)
    最小值为-78.3323
